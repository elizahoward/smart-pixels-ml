{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 14:49:15.074555: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-07 14:49:15.077332: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-07 14:49:15.108466: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-07 14:49:15.108488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-07 14:49:15.109491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-07 14:49:15.114813: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-07 14:49:15.115208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-07 14:49:15.763298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/daq/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "from Classes import FilterClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/elizahoward/smart-pixels-ml/ouput_filtering/tfrecords_train_fba23659 is removed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]\n",
      "Saving batches as TFRecords:   0%|          | 0/107 [00:00<?, ?it/s]2024-10-07 14:49:19.525904: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Saving batches as TFRecords: 100%|██████████| 107/107 [00:01<00:00, 65.10it/s]\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training generator 3.108513832092285 seconds ---\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 13, 21, 2)]       0         \n",
      "                                                                 \n",
      " q_separable_conv2d (QSepar  (None, 11, 19, 5)         33        \n",
      " ableConv2D)                                                     \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 11, 19, 5)         0         \n",
      "                                                                 \n",
      " q_conv2d (QConv2D)          (None, 11, 19, 5)         30        \n",
      "                                                                 \n",
      " q_activation_1 (QActivatio  (None, 11, 19, 5)         0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1045)              0         \n",
      "                                                                 \n",
      " q_dense (QDense)            (None, 1)                 1046      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1109 (4.33 KB)\n",
      "Trainable params: 1109 (4.33 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "--- Model create and compile 0.989915132522583 seconds ---\n",
      "Compiling model with learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/elizahoward/cmspix28-mc-sim/MuC_Output/\"\n",
    "model = FilterClusters(data_directory_path = data_path, labels_directory_path=data_path, tag = '', include_y_local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model with learning rate: 0.001\n",
      "Epoch 1/10\n",
      "107/107 [==============================] - 10s 82ms/step - loss: 3.5593 - binary_accuracy: 0.6331 - val_loss: 5.9545 - val_binary_accuracy: 0.4892\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 8s 78ms/step - loss: 1.8817 - binary_accuracy: 0.7214 - val_loss: 3.7707 - val_binary_accuracy: 0.4783\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 8s 77ms/step - loss: 1.4507 - binary_accuracy: 0.7048 - val_loss: 4.7395 - val_binary_accuracy: 0.4778\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 8s 78ms/step - loss: 1.7592 - binary_accuracy: 0.6699 - val_loss: 2.3573 - val_binary_accuracy: 0.4778\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 8s 78ms/step - loss: 4.8402 - binary_accuracy: 0.5264 - val_loss: 1.1685 - val_binary_accuracy: 0.5224\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 8s 78ms/step - loss: 3.5773 - binary_accuracy: 0.5018 - val_loss: 1.0789 - val_binary_accuracy: 0.5546\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 8s 78ms/step - loss: 1.0520 - binary_accuracy: 0.5706 - val_loss: 1.7232 - val_binary_accuracy: 0.4943\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 8s 78ms/step - loss: 0.9950 - binary_accuracy: 0.6496 - val_loss: 1.4019 - val_binary_accuracy: 0.4885\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 8s 78ms/step - loss: 1.0085 - binary_accuracy: 0.5096 - val_loss: 0.9145 - val_binary_accuracy: 0.5622\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 8s 78ms/step - loss: 0.9505 - binary_accuracy: 0.5353 - val_loss: 0.9139 - val_binary_accuracy: 0.5379\n"
     ]
    }
   ],
   "source": [
    "model.compileModel(learning_rate=0.001)#\n",
    "model.runTraining(epochs=10)\n",
    "#model.loadWeights(\"/home/elizahoward/smart-pixels-ml/weights/weightsBIBsigPrediction.hdf5\")\n",
    "#model.loadWeights(\"/home/elizahoward/smart-pixels-ml/weights/weightsSigPrediction.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 4s 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:03<00:00, 30.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Signal Efficiency: 64.34509803921569%\n",
      "Background Rejection: 44.045355054002655%\n",
      "\n",
      "Overall Accuracy: 53.74468324308119%\n",
      "Fraction of Data that are Signal: 47.78054675935468%\n"
     ]
    }
   ],
   "source": [
    "#model.loadWeights(\"/home/elizahoward/smart-pixels-ml/ouput_prediction/weights/weights.03-t3.61-v3.20.hdf5\")\n",
    "model.checkAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
